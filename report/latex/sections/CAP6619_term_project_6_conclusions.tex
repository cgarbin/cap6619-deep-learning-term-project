\documentclass[../CAP6619_term_project_cgarbin.tex]{subfiles}

\begin{document}

Dropout is a popular regularization strategy. Batch Normalization is frequently used with deep neural networks to mitigate the gradient vanishing problem and also has a regularization effect. In this respect, Dropout and Batch Normalization overlap in some applications. Guidelines to use them are sometimes contradicting and often lacking in details.

This report examined the application of Dropout and Batch Normalization in multilayer perceptron networks (MLPs) and convolutional neural networks (CNNs), both in isolation and together. A network that does not use Dropout or Batch Normalization was used as a baseline.

The goal of the experiments was to analyze combinations of hyperparameters mentioned in the Dropout paper \cite{Srivastava2014} and the Batch Normalization paper \cite{Ioffe2015}, both separately (only Dropout or only Batch Normalization) or in combination (both Dropout and Batch Normalization).

The MLP experiments showed that:

\begin{itemize}
\item Training with Dropout and Batch Normalization is slower, as expected. However, Batch Normalization turned out to be significantly slower, increasing training time by over 80\%.
\item A non-adaptive optimizer (SGD) can outperform an adaptive optimizer (RMSProp). But to do so it required experimentation with other hyperparameters (learning rate, momentum, max-norm), consuming more training time. As a general guideline tests should start with an adaptive optimizer because it will perform better with default parameters. Switching to a non-adaptive optimizer should be reserved for a later phase, when other major decisions have been made (e.g. validate the dataset, explore different network architectures, etc.).
\item Test (prediction) time of a network trained with Batch Normalization is approximately 30\% slower. This may be a factor for some applications because it also results in more energy use, draining batteries faster. This was an unexpected result of the tests and needs further validation.
\end{itemize}

The CNN tests showed that:

\begin{itemize}
\item Adding Batch Normalization improved accuracy without other observable side effects. Since it can be added without major structural changes to the network architecture, adding Batch Normalization should be one of the first steps taken to optimize a CNN.
\item Increasing the learning rate, as recommended in the Batch Normalization paper \cite{Ioffe2015} improves accuracy by 2 to 3\%. Since this is a simple step to take, it should be done in the initial optimization steps, before investing time in more complex optimizations.
\item Adding Dropout reduced accuracy significantly. This could be a deficiency of the experiments conducted here because other sources reported improvements when Dropout was used. At a minimum, it is a cautionary sign that using Dropout in CNNs require careful consideration. As a practical suggestion, remove all Dropout layers from the network and test it again to check it is not harming performance.
\end{itemize}

\end{document}

% Conclusions: In the conclusions, you should briefly summarize the research problem
% studied in your report. Explain what you have done and summarize the major findings.
% Draw any informative conclusions, which can be useful to guide the followers [150-200
% words, 1 point]
