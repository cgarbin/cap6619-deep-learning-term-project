\documentclass[../CAP6619_term_project_cgarbin.tex]{subfiles}

\begin{document}
\subsection{Generalization and Dropout}

The fundamental problem in machine learning is \textit{generalization}, the accuracy of a trained model when it evaluates previously unseen data \cite{Goodfellow2016}. \textit{Overfitting} happens when the model performs well on the training data, but performs poorly on new data, i.e. the model has low training error and high test error. \textit{Regularization} is a set of techniques used to reduce overfitting. Over the years many such techniques have been developed. Some of them act on the gradient descent optimization algorithms \cite{Ruder2016}, others act on the input data, artificially creating new training data \cite{Perez2017}.

Other techniques go beyond acting on only one model. One such technique is \textit{model ensembling}, combining the output of several models, each trained differently in some respect, to generate one final answer. It has dominated recent machine learning competitions \cite{Goodfellow2016}. Although model ensembling performs well, it requires a much larger training time by definition (compared to training only one model). Each model in the ensemble has to be trained either from scratch or derived from a base model in significant ways. 

\textit{Dropout} \cite{Srivastava2014} simulates the model ensembling without creating multiple networks. It has been widely adopted since its publication, in part because it doesn't require fundamental changes to the network architecture, other than adding the Dropout layers. 

Despite its simplicity, Dropout still requires tuning of hyperparameters to work well in different applications. The original paper \cite{Srivastava2014} mentions the need to change the learning rate, weight decay, momentum, max-norm, number of units in a layer, among others. Getting Dropout to work well for a given network architecture and input data requires experimentation with these hyperparameters. Adding Dropout to a network increases the convergence time\cite{Srivastava2014}. Then, after adding Dropout, we need to train models with different combinations of hyperparameters that affect its behavior, further increasing training time.

Another, more significant complicating factor is that it was tested with the standard stochastic descent gradient (SGD) optimizer (as it's done in most papers \cite{Ruder2016}). Most networks today use adaptive optimizers, e.g. RMSProp \cite{Tieleman2012}, commonly used in Keras examples. Some of the recommendations in the paper, for example learning rates and weight decay values, do not necessarily apply when an adaptive optimizer is used.

This leads us to techniques to help reduce the training time by helping the models converge faster. One such technique is \textit{Batch Normalization} \cite{Ioffe2015}.

\subsection{Training time and Batch Normalization}

Before \textit{Batch Normalization} \cite{Ioffe2015} was introduced, the time to train a network to converge depended significantly on careful initialization of hyperparameters (e.g. initial weight values) and on the use of small learning rates, which lengthened the training time. The learning process was further complicated by the dependency of one layer on its preceding layers. Small changes in one layer could be amplified when flowing through the other network layers. Batch Normalization significantly reduces training time by normalizing the input of each layer in the network, not only the input layer. This approach allows the use of higher learning rates, which in turn reduces the number of training steps the network need to converge (the original paper reported 14 times fewer steps in some cases).

Similar to Dropout, using Batch Normalization is simple: add Batch Normalization layers in the network. Because of this simplicity, using Batch Normalization would be a natural candidate to be used to speed up training different combinations of hyperparameters needed to optimize the use of Dropout layers (it would not speed up overall training, but would help converge faster).

However, Batch Normalization also has a regularization effect that renders Dropout unnecessary in some cases, as documented in the original paper \cite{Ioffe2015}, in \cite{Luo2018} and \cite{Kohler2018}.

\subsection{The intersection of Dropout and Batch Normalization}

With the overlapping and sometimes contradicting recommendations for Dropout and Batch Normalization usage, choosing the best architecture for a network, one that can be trained in a short amount of time and generalizes well, now becomes a three-part question:

\begin{enumerate}
\item Should it use Dropout or Batch Normalization? Both claim to regularize the network, but do they regularize equally well, and at the same cost of training time and network size?
\item Should it use both Dropout and Batch Normalization? Despite the claim in \cite{Ioffe2015}, other experiments showed that they can be used together to improve a network \cite{Li2018}.
\item Should it use any of them? Could an adaptive optimizer (e.g. RMSProp) be enough to quickly converge to an acceptable accuracy for some problem spaces and input data?
\end{enumerate}

And then, if Dropout should be used, there is one more question to answer:

\begin{enumerate}
\item What values should be used for the hyperparameters that affect Dropout (learning rate, weight decay, momentum, optimizer, etc.)?
\end{enumerate}

We conducted experiments to derive some guidelines for using Dropout and Batch Normalization. The experiments were performed in image classifications tasks (MNIST and CIFAR-10) using multilayer perceptron networks (MLP) and convolutional neural networks (CNN).

The experiments tested networks without Dropout or Batch Normalization to create a baseline, followed by networks only with Dropout, only with Batch Normalization and with both. Each network was further tested with a combination of hyperparameters. The hyperparameters selected for the tests are the ones mentioned in the Dropout paper \cite{Srivastava2014} and the Batch Normalization paper \cite{Ioffe2015}.

\end{document}

% Introduction: Your report should have an introduction section with 500 â€“ 1000 words.
% The introduction should clearly state (1) what is the research problem to be studied
% in the report; (2) the motivation of the problem studied in your report; (3) how are
% the problem solved by existing methods, if any; and (4) a brief description about the
% method you will propose in the report. You should cite at least 8 relevant references
% (publications) in the introduction. If your report is about literature review, you
% will need to cite at least 15 references in the Introduction [500-1000 words: 2 pts]