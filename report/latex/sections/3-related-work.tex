\documentclass[../dropout-vs-batch-normalization.tex]{subfiles}

\begin{document}
\subsection{Parameter Settings for Deep Learning}
Due to the complex nature of the underlying deep learning architecture and the large number of parameters involved in the training, finding proper parameter settings has always been a practical challenge for the deep learning community. In order to optimize gradient-based training of deep architectures, Bengio~\cite{Bengio2012} introduced a practical guide to optimizing hyperparameters. It emphasizes the need to choose a good learning rate as the main decision when optimizing networks.

Because the majority of deep learning architectures rely on gradient descent principle for optimization and a variety of approaches have been proposed, a literature review~\cite{Ruder2016} summarizes the different optimizers and documents guidelines to choose one. The survey also recommends the use of adaptive learning-rate optimizers for sparse data, and names RMSProp, Adadelta and Adam as good choices, with Adam slightly outperforming RMSProp towards the end of the optimization. It also notes that many recent papers use a simple SGD optimizer (no momentum and only simple learning rate annealing schedules), even though an adaptive optimizer could have been a better choice. This points to the need for more investigations of results published in papers using SGD as the optimizer.

Despite the recommendation to start with an adaptive learning-rate optimizer from~\cite{Ruder2016}, a well-tuned SGD optimizer may outperform an adaptive optimizer in some applications. However, this can only be determined by trying out network configurations with the SGD optimizer. \cite{Smith2018} makes several recommendations to help speed up the training phase. Specifically for SGD optimizers, it makes recommendations for learning rate, momentum, and weight decay, all major contributors to the convergence time. It recommends the use of cycling learning rates with cyclical momentum and a quick grid search to determine the weight decay, in all cases closely watching the test and validation losses of the network to prune inefficient combinations of values early on in the training cycle. 

Even with guidelines to choose the initial values for hyperparameters (\cite{Bengio2012}) and guidelines to tune parameters during training (\cite{Smith2018}), verifying that those initial values and modifications during training are indeed helping the network converge faster takes time. In \cite{Hinz2018} the authors propose to start the training phase with a lower dimension version of the input data. For example, when training with images, first start with images resized to a smaller resolution, then increase the resolution in later phases, once it's confirmed that the hyperparameter settings are helping the network to converge. The results prove that this method reduces the time to verify that the network is converging (or not). Since it does not overlap with other hyperparameter tuning strategies, it can be used as a complementary approach to those strategies.

\textcolor{red}{we need to add two or three papers to elaborate empirical study about parameter settings for deep learning. (Christian) Added two papers, \cite{Hinz2018} and \cite{Smith2018} Please let me know if this is sufficient. If so, I'll delete this note.
}


\subsection{Dropout and Batch Normalization}
Because deep learning architectures often have a large number of weight values for tuning, whereas the training process only has a limited number of samples, overfitting becomes a significant challenge. On the other hand, existing approaches to avoid overfitting, such as decision tree pruning or constrained optimization, are either too specific or too expensive, therefore cannot be applied to general deep learning frameworks. Finding simple and effective approaches to avoid overfitting for deep learning is a practical challenge. 

To prevent neural networks from overfitting, Srivastava et al.~\cite{Srivastava2014} introduced Dropout and described how specific hyperparameters (learning rate, momentum, max-norm, etc.) affect its behavior. Appendix A has the recommendations to train networks using Dropout. Most of the recommendations are given in ranges of values for each hyperparameter. For example increase learning rate by 10 to 100 times, use momentum between 0.95 and 0.99, apply max-norm with values from 3 to 4, etc. The dropout rate itself is recommended as a range between 0.5 and 0.8 (for hidden layers). Mixing this number of hyperparameters and their ranges result in a large matrix of combinations to try during training.

To accelerate the deep network training Ioffe and Szegedy~\cite{Ioffe2015} introduced Batch Normalization. It shows that Batch Normalization enables higher learning rates by reducing internal covariate shift, but doesn't prescribe a value or a range to be used. It also recommends to reduce L2 weight regularization and to accelerate learning rate decay. Finally, it recommends removing Dropout altogether and count on the regularization effect provided by Batch Normalization. This claim has been studied in newer articles (some of them are referenced below). These newer investigations resulted in recommendations to use Dropout together with Batch Normalization in some scenarios.

Noticing the success of the Dropout and Batch Normalization for deep learning, \cite{Li2018} reconciles Dropout and Batch Normalization for some applications and shows that combining them reduces the error rate in those applications. Its specific recommendation is to apply Dropout after Batch Normalization, with a small Dropout rate. 

In order to better understand regularization in Batch Normalization, \cite{Luo2018} shows that using Dropout after a Batch Normalization layer is beneficial if the batch size is large (256 samples or more) and a small (0.125) dropout rate is used (similar to the findings in \cite{Li2018} in this respect). It also hypothesizes that Dropout did not work in \cite{Ioffe2015} because it was tested with a small batch size.

\textcolor{red}{In summary, while a handful of research has empirically studied dropout and batch normalization, and recommends some practical settings for each of them, our research differs from existing works in the following aspects..... (Christian) Please see next paragraph.}

In summary, while there is empirical research of Dropout and Batch Normalization, with some practical recommendations for settings for each of them, our research differs from existing works in the following aspects. It investigates not only the effect of the Dropout and Batch Normalization layers, but also how they behave with different optimizers: the SGD optimizer, commonly used in publications, and an adaptive optimizer (RMSProp), commonly used in commercial applications. It also investigates the efficiency of the networks when Dropout and Batch Normalization are used, measured in terms of the time to train the networks, the time a trained network takes to predict values and how much memory the network uses. These factors are important for real-life applications, where the best possible accuracy is not the only deciding factor to adopt a network architecture.

\end{document}
