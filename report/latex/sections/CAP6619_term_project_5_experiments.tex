\documentclass[../CAP6619_term_project_cgarbin.tex]{subfiles}

\begin{document}

This section describes the setup used for the experiments and the results collected from them.

The first subsection describes the experiment setup. The second subsection describes the experiments and their results using multilayer perceptron networks (MLP). The third section does the same for convolutional neural networks (CNNs).

\subsection{Experimental Settings}

\medskip
\textbf{Test environment}

Tests were executed on a Google Cloud virtual machine with the following specification:

\begin{itemize}
\item Machine type: n1-standard-4
\item Number of CPUs: 4
\item Memory: 15 GB
\item GPU: 1 x NVIDIA Tesla P100
\end{itemize}

The base image used for this virtual machine was \textit{Intel\textsuperscript{\textregistered} optimized Deep Learning image}, described by Google Cloud as \textit{A Debian based image with TensorFlow (With CUDA 10.0 and Intel\textsuperscript{\textregistered} MKL-DNN, Intel\textsuperscript{\textregistered} MKL) plus Intel\textsuperscript{\textregistered} optimized NumPy, SciPy, and scikit-learn.}

Using a GPU was essential to explore the combination of hyperparameters described in the following sections. Training was 30x faster in that environment, compared to a relatively high-performing machine without a CPU (Intel i7 2.9 GHz, 16 GB RAM).

\medskip
\textbf{Tools}

The following tools were used in the experiments:

\begin{itemize}
\item Python 3.5.3
\item Keras 2.2.4
\item Tensorflow 1.12.0 (with GPU support enabled)
\end{itemize}

\subsection{Results for Multilayer Perceptron Networks}

This section describes the experiments performed with multilayer perceptron networks and reviews the results. First the details of the experiments are described, then the results are analyzed.

MLPs are the networks that have only dense layers (no convolution or other operation is applied).

\medskip
\textbf{Baseline}

Since the goal of these experiments is to compare the relative performance of the different configurations tested, the baseline for those experiments is an MLP that does not use Dropout or Batch Normalization. This network is referred to as \textit{standard MLP} in the text.

\medskip
\textbf{Configurations Tested}

Three MLP configurations were tested using the MNIST dataset:

\begin{enumerate}
\item Baseline: a standard MLP, with dense layers connected to each other without Dropout or Batch Normalization. This configuration is used as the baseline.
\item Dropout: added Dropout layers to the standard network, following the guidelines in the Dropout paper \cite{Srivastava2014}.
\item Batch Normalization: added Batch Normalization layers to the standard network, following the guidelines in the Batch Normalization paper \cite{Ioffe2015}.
\end{enumerate}

\medskip
\textbf{Hyperparameters Tested}

Each of these networks was tested with a combination of these hyperparameters:

\begin{itemize}
\item Hidden layers: 2, 3 and 4 hidden layers. These numbers were chosen because they were described in the Dropout paper \cite{Srivastava2014}.
\item Units in each hidden layer: 1,024 and 2,048 units. These numbers were chosen for the same reason as above.
\item Batch size: 128 samples.
\item Epochs, 5, 20 and 50 epochs.
\item Optimizer: a non-adaptive stochastic gradient descent (SGD) optimizer and RMSProp \cite{Tieleman2012}. The SGD optimizer was chosen because there are indications that most published results use such an optimizer \cite{Ruder2016}. The RMSProp optimizer was chosen to compare the performance of SGD with an adaptive optimizer.
\item Activation function: ReLU \cite{Nair2010} in all cases. It was chosen because the Dropout paper \cite{Srivastava2014} and the Batch Normalization paper \cite{Ioffe2015} also use it. A small-scale test was performed with sigmoid to test its behavior and found that accuracy was comparable to the ReLU (slightly lower, but not significantly). Because that test did not point to major differences, the investigations proceeded only with ReLU.
\end{itemize}

Besides the list above, each network was also tested with different values for learning rate, weight decay, SGD momentum and max-norm values. These hyperparameters depend on the network being tested. The range of values for them is documented within the respective sections below.


\medskip
\textbf{Standard MLP Network Tests - Baseline}

The standard MLP, without Dropout or Batch Normalization was used as the baseline.

\smallskip
\textit{Network structure}

A standard MLP network in this context is a network made of only dense layers, without any Dropout or Batch Normalization layer. 

Figure~\ref{fig:MlpStandardModel} shows an example of a standard MLP network used in the tests.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\columnwidth]{figures/figures_mlp/MlpStandardModel.png}}
\caption{Sample standard MLP network used in the tests}
\label{fig:MlpStandardModel}
\end{figure}

\medskip
\textit{Hyperparameters tested}

\begin{itemize}
\item Learning rates for SGD: the default Keras rate 0.01 and a higher rate 0.1 to compare the behavior of this network with the Dropout network (also tested with higher learning rate).
\item Learning rate for RMSProp: the default Keras rate 0.001 and a higher rate 0.002. The higher rate is not as high as the SGD based on experiments. A 10x higher rate resulted in low accuracy in small-scale tests.
\item Decay for SGD: the default Keras value 0.0 (no decay applied) and a small decay 0.001 to compare with the Dropout and Batch Normalization tests where a decay was also applied.
\item Decay for RMSProp: the default Keras value 0.0 (no decay applied) and a small decay 0.00001 to compare with the Dropout and Batch Normalization tests where a decay was also applied. The small decay value was chosen based on values used in the Keras examples, then verified empirically with small-scale tests.
\item Momentum for SGD: the default Keras value of 0.0 (no momentum applied) and the value 0.95, also used in Dropout and Batch Normalization test. Momentum is not applicable to RMSProp.
\item Max-norm constraint: no max-norm and a max-norm constraint with max value set to 2.
\end{itemize}

The combination of these hyperparameters and the hyperparameters applicable to all MLP networks listed above resulted in 288 tests using the SGD optimizer and 144 tests with the RMSProp optimizer.

\medskip
\textit{Best results}

The top 10 results of these tests are listed in table~\ref{tab:MlpStandardTop10} on page~\pageref{tab:MlpStandardTop10}.

% Tip to make table span two columns from https://stackoverflow.com/questions/33971693/latex-ieee-template-use-single-column-table-in-multicolumn-latex-content

\begin{table*}
\centering

\caption{Top 10 test accuracy for the standard MLP network (no Dropout or Batch Normalization)}
\label{tab:MlpStandardTop10}

% Reduced space between columns from https://tex.stackexchange.com/questions/201105/reduce-cell-margins-in-a-table
\setlength\tabcolsep{2pt}

\begin{tabular}{lrccrcrrccrrr}
\thead{Optimizer} & \thead{Test\\accuracy} & \thead{Hidden\\layers} & \thead{Units per\\layer} & \thead{Epochs} & \thead{Batch\\size} & \thead{Learning\\rate} & \thead{Decay} & \thead{SGD\\moment.} & \thead{max-\\norm} & \thead{Parameters\\count} & \thead{Training\\time (s)} & \thead{Test\\time (s)} \\
\hline

SGD & 0.9879 & 2 & 2048 & 50 & 128 & 0.1 & 0 & 0.95 & none & 5,824,522 & 137 & 0.609 \\
SGD & 0.9878 & 3 & 2048 & 50 & 128 & 0.1 & 0 & 0.95 & 2 & 10,020,874 & 165 & 0.677 \\
SGD & 0.9869 & 3 & 2048 & 50 & 128 & 0.1 & 0 & 0.95 & none & 10,020,874 & 167 & 0.650 \\
SGD & 0.9866 & 4 & 2048 & 50 & 128 & 0.1 & 0 & 0.95 & none & 14,217,226 & 191 & 0.696 \\
SGD & 0.9865 & 2 & 2048 & 5 & 128 & 0.1 & 0.001 & 0.95 & none & 5,824,522 & 17 & 0.704 \\
SGD & 0.9864 & 4 & 2048 & 50 & 128 & 0.1 & 0 & 0.95 & 2 & 14,217,226 & 191 & 0.698 \\
SGD & 0.9863 & 2 & 1024 & 50 & 128 & 0.1 & 0 & 0.95 & none & 1,863,690 & 125 & 0.584 \\
SGD & 0.9861 & 3 & 2048 & 50 & 128 & 0.1 & 0.001 & 0.95 & 2 & 10,020,874 & 170 & 0.787 \\
RMSprop & 0.9860 & 2 & 2048 & 20 & 128 & 0.001 & 0.00001 & 0 & 2 & 5,824,522 & 69 & 0.644 \\
SGD & 0.9859 & 3 & 1024 & 50 & 128 & 0.1 & 0 & 0.95 & 2 & 2,913,290 & 135 & 0.583 \\

\end{tabular}
\end{table*}

\medskip
\textbf{MLP Network with Dropout}

\smallskip
\textit{Network structure}

The MLP Dropout network was modeled after the Dropout paper \cite{Srivastava2014}. The significant changes compared to the standard MLP network are:

\begin{itemize}
\item A Dropout layer was added after the input layer, using a lower dropout rate (compared to the dropout rate in Dense layers).
\item A Dropout layer was added after each dense layers, with a higher dropout rate, also as recommended.
\end{itemize}

Figure~\ref{fig:MlpDropoutModel} shows an example of a Dropout network used in the tests.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\columnwidth]{figures/figures_mlp/MlpDropoutModel.png}}
\caption{Sample Dropout MLP network used in the tests}
\label{fig:MlpDropoutModel}
\end{figure}

\medskip
\textit{Hyperparameters tested}

\begin{itemize}
\item Dropout rate for the input layer: 0.1, as recommended in \cite{Srivastava2014}.
\item Dropout rate for hidden layers: 0.5, the high end of the rate recommended in \cite{Srivastava2014}.
\item Learning rates for SGD: the default Keras rate 0.01 and 10x the default rate (0.1) as recommended in \cite{Srivastava2014}.
\item Learning rate for RMSProp: the default Keras rate 0.001 and a higher rate 0.01. Although the standard MLP network did not use such a high rate, the 10x rate was used here to test the recommendation in \cite{Srivastava2014}.
\item Decay for SGD: the default Keras value 0.0 (no decay applied) and a small decay 0.001 to compare with the Dropout and Batch Normalization tests where a decay was also applied.
\item Decay for RMSProp: the default Keras value 0.0 (no decay applied) and a small decay 0.00001 to compare with the Dropout and Batch Normalization tests where a decay was also applied. The small decay value was chosen based on values used in the Keras examples, then verified empirically with small-scale tests.
\item Momentum for SGD: the default Keras value of 0.0 (no momentum applied) and the two extremes of the range recommended in \cite{Srivastava2014}: 0.95 and 0.99. Momentum is not applicable to RMSProp.
\item Max-norm constraint: no max-norm to test the behavior of the network when using the same value as the standard MLP and two of the values recommended in \cite{Srivastava2014}: 2 and 3.
\end{itemize}

The combination of these hyperparameters and the hyperparameters applicable to all MLP networks listed above resulted in 648 tests using the SGD optimizer and 216 tests with the RMSProp optimizer.

\medskip
\textit{Best results}

The top 10 results of these tests are listed in table~\ref{tab:MlpDropoutTop10} on page~\pageref{tab:MlpDropoutTop10}.

\begin{table*}
\centering

\caption{Top 10 test accuracy for the Dropout MLP network}
\label{tab:MlpDropoutTop10}

% Reduced space between columns from https://tex.stackexchange.com/questions/201105/reduce-cell-margins-in-a-table
\setlength\tabcolsep{2pt}

\begin{tabular}{lrccrcccrrccrrr}
\thead{Optimizer} & \thead{Test\\accuracy} & \thead{Hidden\\layers} & \thead{Units per\\layer} & \thead{Epochs} & \thead{Batch\\size} & \thead{Dropout\\rate input} & \thead{Dropout\\rate hidden} & \thead{Learning\\rate} & \thead{Decay} & \thead{SGD\\moment.} & \thead{max-\\norm} & \thead{Parameters\\count} & \thead{Training\\time (s)} & \thead{Test\\time (s)} \\
\hline

SGD & 0.9881 & 2 & 2048 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0.001 & 0.99 & 3 & 20,037,642 & 244 & 0.701 \\
SGD & 0.9879 & 3 & 2048 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0 & 0.95 & 3 & 36,818,954 & 335 & 0.779 \\
SGD & 0.9879 & 4 & 2048 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0.001 & 0.99 & 2 & 53,600,266 & 429 & 0.808 \\
SGD & 0.9876 & 3 & 1024 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0 & 0.95 & none & 10,020,874 & 180 & 0.725 \\
SGD & 0.9876 & 4 & 2048 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0.001 & 0.99 & 3 & 53,600,266 & 429 & 0.826 \\
SGD & 0.9875 & 2 & 1024 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0.001 & 0.99 & none & 5,824,522 & 161 & 0.662 \\
SGD & 0.9875 & 2 & 2048 & 50 & 128 & 0.1 & 0.5 & 0.1 & 0 & 0 & 3 & 20,037,642 & 241 & 0.730 \\
SGD & 0.9875 & 4 & 1024 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0 & 0.95 & none & 14,217,226 & 204 & 0.671 \\
SGD & 0.9874 & 2 & 2048 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0 & 0.95 & 2 & 20,037,642 & 242 & 0.722 \\
SGD & 0.9874 & 3 & 2048 & 50 & 128 & 0.1 & 0.5 & 0.01 & 0.001 & 0.99 & 3 & 36,818,954 & 340 & 0.771 \\

\end{tabular}
\end{table*}

\medskip
\textbf{MLP Network with Batch Normalization}

\smallskip
\textit{Network structure}

The MLP Batch Normalization network was modeled after the Batch Normalization paper \cite{Ioffe2015}. The significant change compared to the standard MLP network is the addition of a Batch Normalization layer after the dense layers.

Figure~\ref{fig:MlpBatchNormalizationModel} shows an example of a Batch Normalization network used in the tests.

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\columnwidth]{figures/figures_mlp/MlpBatchNormalizationModel.png}}
\caption{Sample Batch Normalization MLP network used in the tests}
\label{fig:MlpBatchNormalizationModel}
\end{figure}

\medskip
\textit{Hyperparameters tested}

\begin{itemize}
\item Learning rates for SGD: the default Keras rate 0.01 and 10x the default rate (0.1). \cite{Ioffe2015} recommends a higher learning rate, but does not give a range of values. These values were chosen to match the ones used in the Dropout MLP tests, making the comparison more meaningful.
\item Learning rate for RMSProp: the default Keras rate 0.001 a higher rate 0.005. Although the standard MLP network did not use such a high rate, \cite{Ioffe2015} recommends a higher rate.
\item Decay for SGD: the default Keras value 0.0 (no decay applied) and a small decay 0.001 to compare with the other tests where a decay was also applied.
\item Decay for RMSProp: the default Keras value 0.0 (no decay applied) and a small decay 0.00001 to compare with the others tests where a decay was also applied. The small decay value was chosen based on values used in the Keras examples, then verified empirically with small-scale tests.
\item Momentum for SGD: the default Keras value of 0.0 (no momentum applied) and the two extremes of the range recommended in \cite{Srivastava2014}: 0.95 and 0.99.  Momentum is not applicable to RMSProp.
\item Max-norm constraint: no max-norm to test the behavior of the network when using the same value as the standard MLP and two of the values recommended in \cite{Srivastava2014}: 2 and 3.
\end{itemize}

The combination of these hyperparameters and the hyperparameters applicable to all MLP networks listed above resulted in 144 tests using the SGD optimizer and 72 tests with the RMSProp optimizer.

\medskip
\textit{Best results}

The top 10 results of these tests are listed in table~\ref{tab:MlpBatchNormalizationTop10} on page~\pageref{tab:MlpBatchNormalizationTop10}.

\begin{table*}
\centering

\caption{Top 10 test accuracy for the Batch Normalization MLP network}
\label{tab:MlpBatchNormalizationTop10}

% Reduced space between columns from https://tex.stackexchange.com/questions/201105/reduce-cell-margins-in-a-table
\setlength\tabcolsep{2pt}

\begin{tabular}{lrccrcrrcrrr}
\thead{Optimizer} & \thead{Test\\accuracy} & \thead{Hidden\\layers} & \thead{Units per\\layer} & \thead{Epochs} & \thead{Batch\\size} & \thead{Learning\\rate} & \thead{Decay} & \thead{SGD\\moment.} & \thead{Parameters\\count} & \thead{Training\\time (s)} & \thead{Test\\time (s)} \\
\hline

SGD & 0.9868 & 4 & 2048 & 50 & 128 & 0.01 & 0 & 0.95 & 14,243,850 & 394 & 0.933 \\
SGD & 0.9867 & 2 & 1024 & 50 & 128 & 0.1 & 0.0001 & 0.95 & 1,870,858 & 241 & 0.765 \\
RMSprop & 0.9867 & 4 & 2048 & 50 & 128 & 0.001 & 0.0001 & 0 & 14,243,850 & 439 & 0.927 \\
SGD & 0.9865 & 3 & 1024 & 50 & 128 & 0.1 & 0.0001 & 0.95 & 2,923,530 & 293 & 0.857 \\
SGD & 0.9864 & 2 & 2048 & 50 & 128 & 0.1 & 0.0001 & 0.95 & 5,838,858 & 255 & 0.795 \\
SGD & 0.9864 & 4 & 2048 & 50 & 128 & 0.01 & 0.0001 & 0.95 & 14,243,850 & 388 & 0.901 \\
RMSprop & 0.9862 & 3 & 1024 & 20 & 128 & 0.001 & 0.0001 & 0 & 2,923,530 & 125 & 0.868 \\
SGD & 0.9860 & 2 & 2048 & 50 & 128 & 0.01 & 0 & 0.95 & 5,838,858 & 255 & 0.780 \\
SGD & 0.9860 & 4 & 2048 & 20 & 128 & 0.01 & 0 & 0.95 & 14,243,850 & 161 & 0.927 \\
SGD & 0.9859 & 2 & 2048 & 20 & 128 & 0.01 & 0 & 0.95 & 5,838,858 & 101 & 0.764 \\

\end{tabular}
\end{table*}

\medskip
\textbf{Analysis of the results}

\smallskip
\textit{Accuracy}

All networks resulted in similar accuracy, with a small edge for Dropout.

Figure~\ref{fig:MlpLossGraph} shows that the accuracy is not reached at the same time. The figure plots the training (blue, dotted line) and test (solid orange line) loss during training.

The Batch Normalization network reaches its lower value much earlier than other networks, as expected, given that one of its purposes is to accelerate learning \cite{Ioffe2015}. This can be taken advantage of to shorten training times when a very high accuracy is not needed. Early stopping would stop training with the Batch Normalization sooner than with the other networks.

\begin{figure*}
    \centering
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/figures_mlp/MlpStandardLossGraph} 
        \caption{Loss for standard MLP} \label{fig:MlpStandardLossGraph}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/figures_mlp/MlpDropoutLossGraph} 
        \caption{Loss for Dropout MLP} \label{fig:MlpDropoutLossGraph}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \includegraphics[width=\textwidth]{figures/figures_mlp/MlpBatchNormLossGraph} 
        \caption{Loss for Batch Normalization MLP} \label{fig:MlpBatchNormLossGraph}
    \end{subfigure}
    \caption{Loss graphs for the top MLP network in each test}
    \label{fig:MlpLossGraph}
\end{figure*}


\smallskip
\textit{Performance: training and test CPU time, parameter count}

To evaluate the training and test CPU and parameter count, the best results of each network configuration using two hidden layers was extracted into table~\ref{tab:MlpPerformance}.\footnote{Note that the Dropout network is listed in the top 10 results as "1024 hidden units". The number of units is adjusted by the dropout rate, 0.5 in this case. The adjustment results in a Dropout network configured to run with 1024 units in a layer to effectively have 2048 units in that layer.}

It shows these behaviors of the different networks:

\begin{itemize}
\item Training CPU time: Dropout increases training time by approximately 17\%. Batch Normalization increases training time by approximately 86\%. As shown in figure~\ref{fig:MlpTrainingTime}, this increase in training time happens in all combinations of hyperparameters. It is not the product of a specific set of hyperparameters. From that we can make the general statement that Batch Normalization training time is approximately 80\% longer than the standard and Dropout networks.
\item Test CPU time: Batch Normalization is significantly slower (30+\%) at test time. This result is surprising, given that the network architecture is effectively the same. It could be a fluctuation of the environment. It needs some further research. If it does indeed increase the test time by this much, it has significant implications for uses in restricted environments, e.g. mobile phones, were battery conservation is a high-priority concern.
\item Parameter count: as expected, Batch Normalization uses more parameters than the standard MLP and Dropout and therefore more memory (parameter count is used as a proxy for memory usage). However, the increase is small (less than 1\%) and occurs only at training time (where usually more memory is available).
\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.9\columnwidth]{figures/figures_mlp/MlpTrainingTime.png}}
\caption{Training time in seconds for MLP standard, Dropout and Batch Normalization networks using two hidden layers, 2048 units in each layer, trained for 50 epochs. Showing all combination of hyperparameters (learning rate, decay, momentum, etc.). Each dot represents a test executed with the different hyperparameters.}
\label{fig:MlpTrainingTime}
\end{figure}

\begin{table}
\centering

\caption{Performance evaluation comparing networks with two hidden layers and 2048 hidden units (note that Dropout is divided by 0.5, the dropout rate). All tests done with 50 epochs, SGD optimizer, 0.1 learning rate and 0.99 momentum for Dropout, 0.95 for the other networks.}
\label{tab:MlpPerformance}

% Reduced space between columns from https://tex.stackexchange.com/questions/201105/reduce-cell-margins-in-a-table
\setlength\tabcolsep{2pt}

\begin{tabular}{lrrrr}
\thead{Network} & \thead{Accuracy} & \thead{Training\\time (s)} & \thead{Test\\time (s)} & \thead{Parameters\\count} \\
\hline

Standard & 0.9879 & 137 & 0.609 & 5,824,522 \\
Dropout & 0.9875 & 161 & 0.662 & 5,824,522 \\
Batch Normalization & 0.9864 & 255 & 0.795 & 5,838,858

\end{tabular}
\end{table}

\smallskip
\textit{Effects of some combinations of hyperparameters}

Inspecting the top 10 results for each network reveals some patterns.

The non-adaptive optimizer SGD performed unexpectedly well compared to the adaptive RMSProp. However, changing the default learning rate and adding momentum were needed to achieve that performance. Adding a max-norm constraint was needed in most cases as well.

Getting to that performance level requires experimentation with those hyperparameters, which translates in more test time.


\medskip
\textbf{Recommendations based on the results}

\begin{itemize}
\item Use Batch Normalization if the convergence time is more important than absolute accuracy. Together with early stopping, it could significantly reduce training time.
\item But be aware of Batch Normalization's training time increase. Unless it can be shown that it is helping converge faster during training, it may not be worth using it for experiments. Each experiment will take significantly longer to complete. It may be better to start with a standard network to run experiments faster, then switch to Batch Normalization in a later phase.
\item Start with an adaptive optimizer (e.g. RMSProp). The experiments show that a non-adaptive SGD optimizer can be fine-tuned to outperform an adaptive one, but that comes at the cost of trying combinations of hyperparameters to find one that performs well. This adds to the training time. The accuracy of the adaptive optimizer with its default settings is not much lower. Starting with that configuration quickly provides a baseline for the tests and frees up time to experiment with other hyperparameters (e.g. the number of hidden layers, batch size, etc.). 
\end{itemize}


\medskip
\textbf{Future investigations}

Considering the results so far and what was learned in producing this report, these are some improvements that could be done in the experiments and data collection process:

\begin{itemize}
\item Batch Normalization test time validation: tests showed that that Batch Normalization test time is significantly higher than the standard MLP and Dropout. This is unexpected and warrants more investigations.
\item Force overfitting in each test: to better evaluate the effect of the hyperparameters, the test should begin by verifying that overfitting is taking place and where it does so (which epoch). Forcing overfitting would have triggered more differences in accuracy across the network types, providing more actionable recommendations for the readers. Once the network is overfitting we can verify if the hyperparameter changes resolve the overfitting and how soon it does so (which epoch). A possible way to force overfitting in these tests is to reduce the number of samples in the training set.
\item Effect of different dropout rates: the tests were performed with the dropout rates recommended in \cite{Srivastava2014} because of the limited amount of time. Since the dropout rate is a key hyperparameter, another investigation path could be to explore its effect on the top 10 results (e.g. could we improve the Dropout network further with different rates?).
\item Capture \href{https://www.tensorflow.org/guide/summaries_and_tensorboard}{tensorboard} data: Keras can save data during training into a format that \href{https://www.tensorflow.org/guide/summaries_and_tensorboard}{tensorboard} can read. Making the data available in this format allows a deeper, more detailed exploration of the results by other readers, potentially resulting in more insights.
\end{itemize}






%------------------------------------------------------------------

\subsection{Results for Convolutional Neural Networks}

This section describes the experiments performed with convolutional neural networks and reviews the results. First the details of the experiments are described, then the results are analyzed.

CNNs are the networks that have convolution and max-pooling layers as their main feature, followed by flattening and dense layers before an output layer.

\medskip
\textbf{Baseline}

Since the goal of these experiments is to compare the relative performance of the different configurations tested, the baseline for those experiments is a CNN that does not use Dropout or Batch Normalization. This network is referred to as \textit{standard CNN} in the text.

\medskip
\textbf{Configurations Tested}

The following CNN configurations were tested:

\begin{enumerate}
\item Baseline: a standard CNN, with convolution and max-pooling layers, without using any Dropout or Batch Normalization layers. This configuration is used as the baseline. This CNN was based on the official Keras example [ref] and similar to the CNN used in \cite{Srivastava2014} for the Google Street View House Numbers tests. Figure~\ref{fig:CnnStandardModel} shows this network configuration.
\item Dropout before the dense layer: starting with the standard CNN, added a Dropout layer right before the dense layer. No other Dropout or Batch Normalization layer was added. Figure~\ref{fig:CnnDropoutDenseModel} shows this network configuration.
\item Dropout after all layers: starting with the standard CNN, added Dropout to the convolutions and also before the dense layer. The Dropout layer was added after the max-pooling layers. Figure~\ref{fig:CnnDropoutAllModel} shows this network configuration.
 \item Batch Normalization: starting with the standard CNN, added Batch Normalization layers between the convolution and the max-pooling layers. Although \cite{Ioffe2015} adds Batch Normalization before the non-linearity, subsequent experiments reported that adding Batch Normalization after the non-linearity improves accuracy \cite{Mishkin2016}. Because of such reports, tests were executed with the Batch Normalization layer after the non-linearity layer. Figure~\ref{fig:CnnBatchNormalizationModel} shows this network configuration.
 \item Dropout + Batch Normalization: a combination of the Dropout and Batch Normalization networks. Figure~\ref{fig:CnnBatchNormDropoutModel} shows this network configuration.
\end{enumerate}

All tests were executed with data augmentation using Keras \verb|ImageDataGenerator| using these transformations:

\begin{itemize}
\item Random vertical shift with a factor of 0.1 (of total height), filling the new pixels with the nearest neighbor.
\item Random horizontal shift with a factor of 0.1 (of total width), filling the new pixels with the nearest neighbor.
\item Random horizontal flipping of images.

\end{itemize}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\columnwidth]{figures/figures_cnn/CnnStandardModel.png}}
\caption{Standard CNN network used in the tests}
\label{fig:CnnStandardModel}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\columnwidth]{figures/figures_cnn/CnnDropoutDenseModel.png}}
\caption{CNN network with Dropout after the dense layer used in the tests}
\label{fig:CnnDropoutDenseModel}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\columnwidth]{figures/figures_cnn/CnnDropoutAllModel.png}}
\caption{CNN network with Dropout in all layers used in the tests}
\label{fig:CnnDropoutAllModel}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\columnwidth]{figures/figures_cnn/CnnBatchNormalizationModel.png}}
\caption{CNN network with Batch Normalization in all layers used in the tests}
\label{fig:CnnBatchNormalizationModel}
\end{figure}

\begin{figure}[htbp]
\centerline{\includegraphics[width=0.5\columnwidth]{figures/figures_cnn/CnnBatchNormDropoutModel.png}}
\caption{CNN network with Batch Normalization and Dropout in all layers used in the tests}
\label{fig:CnnBatchNormDropoutModel}
\end{figure}

\medskip
\textbf{Hyperparameters Tested}

Each of these networks was tested with a combination of the hyperparameters listed below, using the CIFAR-10 dataset.

Because testing a CNN with a meaningful data set such as CIFAR-10, even with a small number of layers, is time-consuming, a small combination of hyperparameters was tested (compared to the MLP test).

\begin{itemize}
\item Optimizer: all tests used RMSProp. Although testing with SGD may have provided a useful contrast between an adaptive optimizer (RMSProp) and a non-adaptive one (SGD), time and resources considerations limited the tests to RMSProp.
\item Learning rate: all CNNs were tested with learning rates 0.0001, the default Keras value. In addition to that the Dropout CNNs were tested with 0.001 (the 10x value recommended in the Dropout paper \cite{Srivastava2014}); Batch Normalization CNNs were tested with 0.0005, a higher rate as recommended in general terms (without providing specific values) in \cite{Ioffe2015}. The standard CNN was tested with 0.001 and 0.0005 for comparison.
\item Units in the dense layer: All CNNs were tested with 512 units in the dense layer, as shown in the official Keras example. In addition to that test, the Dropout CNNs were tested with 1024 units to follow the recommendation in \cite{Srivastava2014} to adjust the number of units based on the dropout rate (0.5 in this case).
\item Epochs: all networks were tested with 50 epochs. This number was chosen to let the networks stabilize and to have a reasonable test execution time. Even with this relatively small number of epochs, training one network in a GPU-enable machine took 20 minutes.
\item Dropout rate: a dropout rate of 0.25 was used after convolution layers and 0.5 after dense layers. A smaller dropout rate for the convolution layers was used as documented in \cite{Srivastava2014}.
\item Activation function: ReLU \cite{Nair2010} in all cases. \cite{Krizhevsky2012} showed that ReLU speeds up the training significantly.
\end{itemize}

\medskip
\textbf{Analysis of the results}

Results from the tests are shown in table~\ref{tab:CnnTestResults}.

\begin{table*}
\centering

\caption{Results from the CNN tests}
\label{tab:CnnTestResults}

% Reduced space between columns from https://tex.stackexchange.com/questions/201105/reduce-cell-margins-in-a-table
\setlength\tabcolsep{2pt}

\begin{tabular}{rlrrrcrr}
\thead{Test} & \thead{Network} & \thead{Test\\accuracy} & \thead{Learning\\rate} & \thead{Units in\\dense layer} & \thead{Epochs} & \thead{Parameters\\count} & \thead{Training\\time (s)} \\
\hline

1 & Standard, no Dropout or Batch Normalization & 0.3226 & 0.001 & 1024 & 50 & 2,436,138 & 2920 \\
2 & Standard, no Dropout or Batch Normalization & 0.6945 & 0.0005 & 512 & 50 & 1,250,858 & 2878 \\
3 & Standard, no Dropout or Batch Normalization & 0.8041 & 0.0001 & 1024 & 50 & 2,436,138 & 2884 \\
4 & Standard, no Dropout or Batch Normalization & 0.5717 & 0.001 & 512 & 50 & 1,250,858 & 2872 \\
5 & Standard, no Dropout or Batch Normalization & 0.8021 & 0.0001 & 512 & 50 & 1,250,858 & 2866 \\
6 & Dropout only for dense layer & 0.7426 & 0.0001 & 512 & 50 & 1,250,858 & 2881 \\
7 & Dropout only for dense layer & 0.1001 & 0.001 & 1024 & 50 & 2,436,138 & 2894 \\
8 & Dropout in all layers & 0.7575 & 0.0001 & 512 & 50 & 1,250,858 & 2904 \\
9 & Dropout in all layers & 0.3226 & 0.001 & 1024 & 50 & 2,436,138 & 2920 \\
10 & Batch Normalization & 0.8447 & 0.001 & 512 & 50 & 1,253,546 & 3185 \\
11 & Batch Normalization & 0.8266 & 0.0001 & 512 & 50 & 1,253,546 & 3174 \\
12 & Batch Normalization & 0.8395 & 0.0005 & 512 & 50 & 1,253,546 & 3189 \\
13 & Dropout + Batch Normalization & 0.8002 & 0.0001 & 512 & 50 & 1,251,498 & 3092 \\
14 & Dropout + Batch Normalization & 0.8087 & 0.001 & 1024 & 50 & 2,436,778 & 3094 \\
15 & Dropout + Batch Normalization & 0.7774 & 0.0005 & 512 & 50 & 1,251,498 & 3083 \\
\end{tabular}
\end{table*}

\smallskip
\textit{Accuracy}

Adding Batch Normalization significantly improves the accuracy. 

Dropout, on the other hand, was always detrimental to accuracy (as used in these experiments - see recommendations for further tests).

\smallskip
\textit{Performance: training CPU time, parameter count}

Adding Batch Normalization increased training time, as expected. However, contrary to the MLP test, adding Batch Normalization did not result in a large increase in training time. It increased training time by approximately 10\%.

\smallskip
\textit{Effects of learning rate}

Increasing the learning yields better accuracy only when Batch Normalization is used. In all other cases it is detrimental to accuracy.

Although not a surprising result for the standard CNN, increasing the learning rate when Dropout is used in all layers (test 9) also resulted in a much lower accuracy. 

\medskip
\textbf{Recommendations based on the results}

\begin{itemize}
\item Add Batch Normalization before attempting other changes: combined with increasing the learning rate (see next item), adding Batch Normalization improved accuracy by a significant value without a significant increase of training time. Because it is simple to add Batch Normalization, it is recommended to add it as a baseline for further improvements in the network performance, before attempting more costly hyperparameter changes.
\item Learning rate value: increase it only when using Batch Normalization. \cite{Ioffe2015} recommends to increase it and it does make a significant difference. Tests 10 and 11 in table~\ref{tab:CnnTestResults} shows that increasing it by 10x improves accuracy by 3\%, without any other change in the test parameters. However, adding it for any other configuration, including Dropout, reduces the accuracy.
\end{itemize}


\medskip
\textbf{Future investigations}

Based on the results collected so far, these are some areas that could be investigated further:

\begin{itemize}
\item The low accuracy of Dropout combined with Batch Normalization: contrary to the tests performed here, \cite{Li2018} reported that Dropout can be used to improve accuracy. Investigating the reasons for the low accuracy could provide more information to understand the interactions between Dropout and Batch Normalization.
\item The low accuracy of Dropout in general: this was perhaps the most unexpected result. Several examples, including the official Keras example, add Dropout to the network. Further tests should explore other dropout rates. Two places to start are increasing the batch size, as recommended in \cite{Luo2018}, and reducing the dropout rate, as recommended in \cite{Li2018}.
\item Add max-norm and momentum: as seen in the MLP results, max-norm and momentum make a difference in the behavior of the network. They were not used in the CNN tests due to the limited time (each CNN test takes 20 minutes in a GPU-enable system). These tests could help explain the low accuracy when Dropout is used.
\item Deeper networks: the CNN used in the tests is relatively shallow. Further tests should be executed in deeper networks to verify these results.
\item Capture \href{https://www.tensorflow.org/guide/summaries_and_tensorboard}{tensorboard} data: same as noted in the MLP recommendation. Making the data available in this format allows a deeper, more detailed exploration of the results by other readers, potentially resulting in more insights.
\end{itemize}

\end{document}

% Experiments: In the experiments, you need to introduce (1) main purpose of the 
% experimental studies; (2) what are the tools used to design the algorithms; (3) what
% are the baseline methods for comparisons; and (4) what are the performance measures
% and data used for empirical studies. You should also use figures and tables to report
% the results collected from your studies, and summarize the experimental results
% [1000-1500 words, 7 points].
% a. Experimental settings: including an introduction of baseline methods, programming
%  tools/languages, the setting of the parameters used for different methods. [1 pt]
%  b. Benchmark data: Provide detailed description about data used for your study,
% including detailed information about the size/dimension of the data.[1 pt]
%  c. Baseline methods: In order to demonstrate the performance of your method, you
% will need to use a baseline approach, and compare the performance of your design with
% the baseline. For example, if you are developing a gender prediction method for
% networked data, you can build a simple classifier, and compare your method with this 
% baseline, which will demonstrate the merits of your method, and validate your
% hypothesis [1 pt]
% d. The results: The detailed results reported in figures/tables with necessary analysis
% and descriptions. You will need to include at least one figure and one table to show
% the results. [2 pts]
% e. Analysis of the Results: Please compare the performance of your method and the 
% baseline approach, and analyze why your method can obtain a good performance. Please
% also add a case study example (e.g. an example of a review report and the predicted
% result from your method) to why this sample was predicted with the reported score [2 
% pts]