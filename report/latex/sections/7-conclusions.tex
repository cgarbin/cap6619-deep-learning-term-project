\documentclass[../dropout-vs-batch-normalization.tex]{subfiles}

\begin{document}

Dropout is a popular regularization strategy. Batch Normalization is frequently used with deep neural networks to mitigate the gradient vanishing problem and also has a regularization effect. In this respect, Dropout and Batch Normalization overlap in some applications. Guidelines to use them are sometimes contradicting and often lacking in details.

This report examined the application of Dropout and Batch Normalization in multilayer perceptron networks (MLPs) and convolutional neural networks (CNNs), both in isolation and together. A network that does not use Dropout or Batch Normalization was used as a baseline.

The goal of the experiments was to analyze combinations of hyperparameters mentioned in the Dropout paper \cite{Srivastava2014} and the Batch Normalization paper \cite{Ioffe2015}, both separately (only Dropout or only Batch Normalization) or in combination (both Dropout and Batch Normalization).



\end{document}
