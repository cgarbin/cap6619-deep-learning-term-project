\documentclass[../Dropout-vs-batch-normalization.tex]{subfiles}

\begin{document}
Overfitting and long periods of training time are two fundamental challenges in multilayered neural network learning and deep learning in particular. To tackle the challenges, Dropout~\cite{Srivastava2014} and Batch Normalization~\cite{Ioffe2015} are two well-recognized approaches to avoid overfitting and reduce the network training time. While both approaches share overlapping design principles, numerous research have shown that they have unique strengths to improve deep learning. Many tools are simplifying these two approaches as a simple function call, allowing flexible stacking to form deep learning architectures. Unfortunately, although there are guidelines to use them, no well-defined set of rules or comprehensive studies to investigate them with respect to data input, network configurations, learning efficiency, and accuracy exists. It is not clear when should users consider using Dropout and/or Batch Normalization, how they should be combined (or used alternatively) to achieve optimized deep learning outcomes.

In this paper we carry an empirical study to investigate the impact of Dropout and Batch Normalization for training deep learning models. We use multilayered dense neural networks and convolutional neural networks (CNN) as the deep learning models, mix Dropout and Batch Normalization to design different architectures and subsequently observe their performance in terms of training time, accuracy etc. The interplay between the network structures, Dropout, and Batch Normalization, allow us to conclude when and how Dropout and Batch Normalization should be considered in deep learning. 

%designs and  design different   and validate their combinations The goal of these experiments is to analyze the accuracy of the different networks and hyperparameters, their usage of system resources during training and during test (prediction) time. This analysis will be used to derive some general guidelines to use and fine-tune these network configurations.
%Overfitting and long training times are fundamental problems in machine learning. Dropout \cite{Srivastava2014} significantly improved overfitting, while Batch Normalization \cite{Ioffe2015} significantly reduced training time.

%These two techniques overlap. Using them is not as simple as adding Dropout layers to improve overfitting and adding Batch Normalization layers to speed up training. Although there are guidelines to use them, there are no well-defined rules that can be applied in all cases, for all network configurations and types of input data.

%Getting them to work well in practice requires informed trial and error of different combinations of hyperparameters. Because there is a large number of combinations of hyperparameters, contradicting results have sometimes been published. For example, the paper introducing Batch Normalization \cite{Ioffe2015} recommends removing Dropout because Batch Normalization has enough of a regularization effect in the network, while subsequent work by \cite{Hendrycks2016} and \cite{Li2018} showed that Dropout can indeed be used together with Batch Normalization to improve results.

%Compounding all these observations is the fact that, while there are hypotheses to explain how Dropout and Batch Normalization improve accuracy and training time, there are no definitive explanations for their inner works \cite{Nalisnick2018} \cite{Bjorck2018} \cite{Santurkar2018}. Resulting again in the need to experiment with different configurations and sample data to validate empirically the theoretical assumptions.

%We conducted experiments to create guidelines for using Dropout and Batch Normalization. The experiments were performed in image classifications tasks (MNIST and CIFAR-10) using multilayer perceptron networks (MLPs) and convolutional neural networks (CNNs).

%The goal of these experiments is to analyze the accuracy of the different networks and hyperparameters, their usage of system resources during training and during test (prediction) time. This analysis will be used to derive some general guidelines to use and fine-tune these network configurations.

\end{document}
