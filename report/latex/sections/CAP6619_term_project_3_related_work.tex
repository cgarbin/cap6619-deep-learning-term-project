\documentclass[../CAP6619_term_project_cgarbin.tex]{subfiles}

\begin{document}

\cite{Bengio2012} is a practical guide to optimize hyperparameters. It emphasizes the need to choose a good learning rate as the main decision when optimizing networks.

\cite{Ruder2016} summarizes the different optimizers and documents guidelines to choose one. It recommends the use of adaptive learning-rate optimizers for sparse data, and names RMSProp, Adadelta and Adam as good choices, with Adam slightly outperforming RMSProp towards the end of the optimization. It also notes that many recent papers use a simple SGD optimizer (no momentum and only simple learning rate annealing schedules), even though an adaptive optimizer could have been a better choice. This points to the need for more investigations of results published in papers using SGD as the optimizer.

\cite{Srivastava2014} introduced Dropout and described how specific hyperparameters (learning rate, momentum, max-norm, etc.) affect its behavior. Appendix A has the recommendation to train networks using Dropout. Most of the recommendations are given in ranges of values for each hyperparameter. For example increase learning rate by 10 to 100 times, use momentum between 0.95 and 0.99, apply max-norm with values from 3 to 4, etc. The dropout rate itself is recommended as a range between 0.5 and 0.8 (for hidden layers). Mixing this number of hyperparameters and their ranges results in a large matrix of combinations to try during training.

\cite{Ioffe2015} introduced Batch Normalization. It shows that Batch Normalization enables higher learning rates, but doesn't prescribe a value or a range to be used. It also recommends to reduce L2 weight regularization and to accelerate learning rate decay. Finally, it recommends removing Dropout altogether and count on the regularization effect provided by Batch Normalization. This claim has been studied in newer articles (some of them are referenced below). These newer investigations resulted in recommendations to use Dropout together with Batch Normalization in some scenarios.

\cite{Li2018} reconciles Dropout and Batch Normalization for some applications and shows that combining them reduces the error rate in those applications. Its specific recommendation is to apply Dropout after Batch Normalization, with a small dropout rate. 

\cite{Luo2018} shows that using Dropout after a Batch Normalization layer is beneficial if the batch size is large (256 sample or more) and a small (0.125) dropout rate is used (similarly to the findings in \cite{Li2018} in this respect). It also hypothesizes that Dropout did not work in \cite{Ioffe2015} because it was tested with a small batch size.

\end{document}

% Your report should have a related work section to summarize works (algorithms)
% which already exist to solve the problem. For example, if you are developing a deep
% convolution neural network for traffic sign recognition, you need to summarize existing
% traffic sign recognition methods, such as how do existing methods solve the problem,
% what are the features and the classification models used in the design, etc. [500-1000
% words: 2 pts]
